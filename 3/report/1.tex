\section{Интернет-математика 2009}

В этом задании мы пользовались готовым датасетом "Интернет-математика 2009"
Для получения результатов мы пользовались библиотекой \texttt{RankLib}.

Данный нам датасет разбивался на train и validate части в соотношении 80/20. Ниже приведены реультаты запусков на различных алгоритмах библиотеки и значение метрики NDCG@20 для множества validate:


\begin{tabular}{|c|c|c|}
	\hline
	
	Название алгоритма & Тип& NDCG@20\\
	\hline
	MART & pairwise & 0.74\\
	RankNet & pairwise & 0.71\\
	RankBoost & pairwise & 0.72\\
	$\lambda$Rank & pairwise & 0.67\\
	RandomForest& pairwise & 0.74\\
	\hline
	AdaRank & listwise & 0.72\\
	$\lambda$MART & listwise & 0.67\\
	ListNet & listwise & 0.72\\
	\hline
	Coordinate Ascent & pointwise & 0.38\\
	\hline
\end{tabular} 

Как видно, лучшие результаты получились на pairwise алгоритмах, чуть хуже показали себя listwise алгоритмы и совсем плохо сработал pointwise алгоритм. 

Лучше всего сработал RandomForest c MART с параметрами 300 мешков слов по 10 деревьев по 10 листов. Эта же модель использовалась для ранжированя тестового датасета (см. файл \texttt{ans.txt}).

\section{Обучение на РОМИП 2008}

Следущим этапом было построение собственного датасета на основе коллекции BY.WEB и оценки релевантности запросов РОМИП 2008.

\subsection{Обработка}

С использованием пайплайна, описанного в отчёте к первому домашнему заданию, была проведена лемматизация текстов документов и запросов после которой были оставлены только слова на чистых русском, белорусском или английском языках, а также целые числа. После этого, также в рамках пайплайна, был поизведен подсчёт различных фич: подокументных, позапросных и метрик для пар запрос-документ.

Последним шагом было построение из насчитанных метрик]датасета, состоящего из векторов для пар запрос-документ, имеющих в оценках запросов РОМИП 2008 значения \texttt{vital} или \texttt{notrelevant}.

Подбробнее и офичах и результатах описано ниже.

\subsection{Фичи}

Следующие фичи были подсчитаны для построения датасета:

\begin{enumerate}
	\item Id запроса (qid)
	\item Длина текста в символах
	\item Длина url в символах
	\item Pagerank
	\item Длина запроса в символах
	\item Длина запроса в словах
	\item Доля слов из запроса, присутствующих в документе
	\item Доля слов из запроса, присутствующих в заголовке
	\item Доля слов не из запроса, лежащие в окне минимальной длины, содержащем все слова из запроса.
	\item BM25
\end{enumerate}

\subsection{Обучение}

Как и прежде, датасет был поделен на train и test в соотношении 80/20.
После этого на train были обучены все перечисленные ранее алгоритмы библиотеки RankLib, а не test была посчитана метрика NDCG@20. Полученные результаты перечислены в таблице ниже.


\begin{tabular}{|c|c|c|}
	\hline
	
	Название алгоритма & Тип& NDCG@20\\
	\hline
	MART & pairwise & 0.39\\
	RankNet & pairwise & 0.34\\
	RankBoost & pairwise & 0.38\\
	$\lambda$Rank & pairwise & 0.35\\
	RandomForest& pairwise & 0.39\\
	\hline
	AdaRank & listwise & 0.387\\
	$\lambda$MART & listwise & 0.388\\
	ListNet & listwise & 0.34\\
	\hline
\end{tabular}

Как и для интернет-математики, лучше всего сработали MART и RandomForest.

\subsection{Важность фич}

В качестве основы для оценки важности фич использовался результат алгоритма RandomForest. Из датасета выбрасывались группы фич, на оставшемся датасете запускался случайный лес, посл чего фиксировалось изменение метрики NDCG@20. Результаты приведены ниже (номерам фич соответсвуют номера из списка выше):

\begin{tabular}{|c|c|}
	\hline
	
	Выброшенные фичи & NDCG@20\\
	\hline
	& 0.39 \\
	\hline
	2& 0.3879 \\
	3& 0.3894 \\
	4& 0.3887 \\
	5& 0.3888 \\
	6& 0.3886 \\
	7& 0.3879 \\
	8& {\bf 0.3793} \\
	9& 0.3876 \\
	10& 0.3869 \\
	7, 8&  {\bf 0.3773} \\
	2, 3, 5, 6& 0.3864 \\
	\hline
\end{tabular} 

Как видно, наибольшее значение имеет фича доли слов в заголовке, совпадающих со словами из запроса, что довольно логично. Если эта метрика ненулевая, это может давать нам сильный сигнал в пользу того, что страница релевантна, поскольку очень часто заголовок содержит ключевые слова документа.

На втором месте расположена фича BM25.

При этом следует заметить, что последняя группа фич, свяанная с длинами, практически не изменяет результат при удалении.

Также стоит заметить, что удаление фичей не улучшило результат, что ожидаемо от алгоритма случайного леса.

\subsection{Тестирование}

Тестирование выполнялось на данных 2009 года.
Для получения векторов фич для пар «запрос — документ» использовался тот же процесс, что и для обучения.
Перед процессом была произведена обработка: среди пар, для которых известна оценка релевантности нужно было выбрать пары, которые возвращает Elasticsearch.

Для обработки написан скрипт нп питоне, для парсинга данных 2009 года использовалась библиотека \texttt{xml.dom.minidom}, для обращения к Elasticsearch — официальные бинды для питона.
Запросы в локальную базу Elasticsearch выполняются быстро, поэтому весь процесс обработки занимает не более 10 секунд.
Для записи документа в формате xml использовался стандартный поток вывода: формат файлов достаточно прост, чтобы для этого не требовалось сторонних библиотек.

Также, в связи с тем, что форматы файлов 2008 и 2009 годов разные (файлы 2008 года содержат урлы как идентификаторы документов, а файлы 2009 года — номера), из Elasticsearch доставались как урлы, так и номера, и итоговый файл приводился в соответствие к формату 2008 года.

У обученного случайного леса результаты на оценках 2009 года сильно лучше по сравнению с результатом валидации на 20\% данных 2008 годом: NDCG@20 получилось 0.5111.
Мы этот результат связываем с тем, что на тестовых данных была предварительно произведена обработка Elasticsearch и брались только документы, по умолчанию более релевантные.
Мы прикладываем результаты почти всех алгоритмов, использованных в предыдущем разделе. Все значения — метрика NDCG@20

\begin{tabular}{|c|c|c|c|}
	\hline
	
	Название алгоритма & Тип&  Train & Test \\
	\hline
	MART & pairwise & 0.39 & 0.51\\
    RankNet & pairwise & 0.34 & 0.49\\
    RankBoost & pairwise & 0.38 & 0.51\\
    $\lambda$Rank & pairwise & 0.35 & 0.49\\
    RandomForest& pairwise & 0.39 & 0.51\\
	\hline
    AdaRank & listwise & 0.387 & 0.51\\
    $\lambda$MART & listwise & 0.388 & 0.51\\
    ListNet & listwise & 0.34 & 0.49\\
	\hline
\end{tabular}
